# 実験レポート 2025-01-13
2025年1月時点の自律AIエージェントがどの程度自律的に実装をできるのかについて実験をした。

2025-01-10~12のプログラミングシンポジウムにおいて「コントラスト」というゲームが紹介された。チャットに貼られたルールを元に曖昧さがないかAIにレビューさせたり、有効な戦略を考えさせたりする遊びをしているうちに、これを自律AIエージェントに実装させてみようというアイデアが生まれた。

「プロシンで話題になったゲームの対戦サーバをプロシン期間中にAIが完成させる」ことができるかどうか？これを検証した。

## 手法

Devin.aiを用いた。指示のやり取りは [CHATLOG.md](CHATLOG.md)に置いた。

## 結果

2025年1月時点ではまだ「プロシンで話題になったゲームの対戦サーバをプロシン期間中にAIが完成させる」を実現することはできなかった。
- フロントエンドは 人間vs人間 モードで交互にプレイすることはできる
- ただし横に移動可能であることを理解しておらず、また「相手のコマを飛び越すことができる」という誤ったルール解釈をしている
- ルールに従って盤面状態を管理するゲームサーバと、着手可能点からランダムに選んでプレイするAIのコードが生成されているが、上記のルール解釈の誤りがあるのでそのまま使える質ではないだろう

## 考察

2025年1月時点では否定的な結論だが、2026年のプロシンでは「プロシンで話題になったゲームの対戦サーバをプロシン期間中にAIが完成させる」ことが出来そう

特に時間が掛かっているところが環境構築なので、それは一旦ちゃんと動くものが作られれば「これを参考にして作って」ですんなり行くようになるはず

AIはブラウザを操作して動作テストができてしまうが故に「ブラウザを操作してテストしよう」としてしまい、そこに結構な実時間が消費されてしまう。
まずUIテストのフレームワークを導入させて、画面操作で確認すべきことは自然言語ではなくテストコードとして記述した方が良さそう

UIが作成されてそれを人間が操作した時や、テストケースを作成させようとした時に、AI側のルール理解の不十分さが明らかになった。
自然言語でやりとりしている間はAI側のルール理解の不十分さに人間の側が気づけなかった。
この種のゲームのような複雑な仕様が存在するケースでは、まずテストケースを生成させてそれを人間が慎重にレビューするのが良さそう
